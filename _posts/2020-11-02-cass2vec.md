---
layout: single
title: "Cass2vec"
excerpt: "Classification of justice decisions with Doc2Vec"
header:
    teaser: https://leoguillaume.github.io/assets/images/2020-11-02-cass2vec/teaser.jpg
categories:
  - project
read_time: true
author_profile: true
toc: true
tags:
  - law
  - doc2vec
  - nlp
share: true
comments: true
---

The DILA (Direction de l'information légale et administrative) is a French administration that makes availables to public data relating to judicial information.

Four databases of court decisions are available [online](https://www.data.gouv.fr/fr/). Here I have exploited the CASS database, which brings together major court decisions and decisions of the Court of Cassation. After data processing, the database exploited in the framework of this project is composed of 97678 judgments.

These judicial decisions are labelled (employment contract, crime, car accident, etc.). The aim of the project is to train a classifier model on this dataset.

The project follows this roadmap:
![image](http://leoguillaume.github.io/assets/images/2020-11-02-cass2vec/roadmap.png)

**You can find the project code in this [repository](https://github.com/leoguillaume/Cass2Vec).**

I opted for a naive approach, consisting in learning vector representations on each of the decisions and classifying these decisions with a very simple algorithm (logistic regression).

# What is Doc2Vec ?

To present Doc2vec, I need to say  few words about word embedding.

## Word embeddings

A word embedding is a representation of a word with a vector. Why a vector ? A vector give a information about concepts behind a word and about links between words. Because words whose meaning is close will have vectors that are close in vector space.

![image](http://leoguillaume.github.io/assets/images/2020-11-02-cass2vec/wordembeddings.png){:height="140%" width="140%"}

To create word embeddings we need to learn vectors on very large dataset of text (more one million in generally) with a deep learning algorithm. These algorithms are based on the distributional hypothesis: words that are used and occur in the same contexts tend to purport similar meanings. In other words, if a word often appears with the same words, they are likely to revolve around the same concept.

The most famous is to create a word embedding is [Word2vec](https://radimrehurek.com/gensim/models/word2vec.html). Word2vec can utilize either of two model architectures to produce a vector representation of words: [continuous bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model#CBOW) (CBOW) or [skip-gram](https://en.wikipedia.org/wiki/N-gram#Skip-gram).

In continuous bag-of-words, the idea is to predict the context giving one random word (e.g. predict I want a glass of orange with juice as input) and in skip-gram model, the idea is to predict the word giving just one random word of context (e.g. predict juice with only glass as input).

## Doc2vec

The concept of Doc2vec is to use a word2vec model and added another vector (Paragraph ID below)doc, like so:
![image](http://leoguillaume.github.io/assets/images/2020-11-02-cass2vec/doc2vec.png){:height="140%" width="140%"}

Here it's the CBOW version of Doc2vec, the one we are using in this project, there is also a skipgram version.

# Results

Once the embeddings are learned (size 256), I use a very basic model, a logistic regression to classify the texts to their respective label. On the training set, after 5 epochs, the model reaches a (weighted) F1-score of 0.58 and 0.37 on the test set. These relatively low results are mainly due to the very large number of different labels (579) and their distribution:
![image](http://leoguillaume.github.io/assets/images/2020-11-02-cass2vec/label_distribution.png){:height="140%" width="140%"}
