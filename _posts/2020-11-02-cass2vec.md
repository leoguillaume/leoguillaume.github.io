---
layout: single
title: "Cass2vec"
excerpt: "Classification of justice decisions with Doc2Vec"
header:
    teaser: https://leoguillaume.github.io/assets/images/2020-11-02-cass2vec/teaser.jpg
categories:
  - project
read_time: true
author_profile: true
toc: true
tags:
  - law
  - doc2vec
  - nlp
share: true
comments: true
---

The DILA (Direction de l'information légale et administrative) is a French administration that makes availables to public data relating to judicial information.

Four databases of court decisions are available [online](https://www.data.gouv.fr/fr/). Here I have exploited the CASS database, which brings together major court decisions and decisions of the Court of Cassation. After data processing, the database exploited in the framework of this project is composed of 97678 judgments.

The project follows the following roadmap:
![image](http://leoguillaume.github.io/assets/images/2020-11-02-cass2vec/roadmap.png)

**You can find all the project code in this [repository](https://github.com/leoguillaume/Cass2Vec).**

I opted for a naive approach, consisting in learning vector representations on each of the decisions and classifying these decisions with a very simple algorithm (logistic regression).

# What is Doc2Vec ?

## Word embeddings

To present Doc2vec, I need to say  few words about word embedding. A word embedding is a representation of a word with a vector. Why a vector ? A vector give a information about concepts behind a word and about links between words. Because words whose meaning is close will have vectors that are close in vector space.

![image](http://leoguillaume.github.io/assets/images/2020-11-02-cass2vec/wordembeddings.png)

To create word embeddings we need to learn vectors on very large dataset of text (more one million in generally) with a deep learning algorithm. These algorithms are based on the distributional hypothesis: words that are used and occur in the same contexts tend to purport similar meanings. In other words, if a word often appears with the same words, they are likely to revolve around the same concept.

The most famous is to create a word embedding is [Word2vec](https://radimrehurek.com/gensim/models/word2vec.html). Word2vec can utilize either of two model architectures to produce a vector representation of words: [continuous bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model#CBOW) (CBOW) or [skip-gram](https://en.wikipedia.org/wiki/N-gram#Skip-gram).

In continuous bag-of-words, the idea is to predict the context giving one random word (e.g. predict I want a glass of orange with juice as input) and in skip-gram model, the idea is to predict the word giving just one random word of context (e.g. predict juice with only glass as input).

## Doc2vec

# Results
